version: "3.9"

services:
  opendevin-backend-engine:
    image: ghcr.io/opendevin/sandbox:latest
    expose:
      - 1337
    ports:
      - 1337:1337
    environment:
      - OLLAMA_HOST=http://ollama-service:11434
      - LLM_API_KEY=""
      - LM_MODEL="claude-3-haiku-20240307"
    networks:
      - opendevin-subnetwork
    command: bash -c "python -m pip install pipenv && python -m pipenv install -v && python -m pipenv shell && uvicorn opendevin.server.listen:app --port 3000"
    tty: true

  opendevin-frontend-app:
    image: ghcr.io/opendevin/sandbox:latest
    expose:
      - 11434
    ports:
      - 11434:11434
    networks:
      - opendevin-subnetwork
    command: bash -c "cd frontend && npm install && npm start"
    tty: true

  opendevin-test:
    image: ghcr.io/opendevin/sandbox:latest
    networks:
      - opendevin-subnetwork
    command: bash -i
    tty: true

networks:
  opendevin-subnetwork:

volumes:
  opendevin-backend-dbstore:
